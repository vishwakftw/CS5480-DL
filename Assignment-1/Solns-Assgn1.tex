\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}

\title{Solutions to the Assignment - 1 : CS5480 - \\
Deep Learning}
\author{Vishwak Srinivasan\\
\texttt{CS15BTECH11043}}
\date{}

\begin{document}
\maketitle

\section*{Question 1}

\subsection*{Part a}
\begin{flushleft}
Method 1:
\begin{lstlisting}[language=Python]
t = torch.Tensor([[1, 2, 3],[4, 5, 6],[7, 8, 9]])
col = t[:,1]
print(col)  # Will print a torch.Tensor of size 3
\end{lstlisting}

Method 2:
\begin{lstlisting}[language=Python]
t = torch.Tensor([[1, 2, 3],[4, 5, 6],[7, 8, 9]])
col = t.view(-1)[1::3]
print(col)  # Will print a torch.Tensor of size 3
\end{lstlisting}

Method 3:
\begin{lstlisting}[language=Python]
t = torch.Tensor([[1, 2, 3],[4, 5, 6],[7, 8, 9]])
col = t.mm(torch.Tensor([0, 1, 0]).view(-1)).view(-1)
print(col)  # Will print a torch.Tensor of size 3
\end{lstlisting}
\end{flushleft}

\subsection*{Part b}
\begin{flushleft}
\begin{itemize}
\item \texttt{Tensor}: A wrapper for storing data. With \texttt{Tensor}s, you can perform operations like arithmetic operations, logical operations, exponentiation and so on. A \texttt{Tensor} cannot be differentiated i.e., is never a part of the \texttt{autograd} graph. \texttt{Tensor}s can be multi-dimensional.
\item \texttt{Variable}: A wrapper around \texttt{Tensor}. By default, the \texttt{requires\_grad} attribute is set to \texttt{false}, which means that the derivatives are not calculated for this variable in the graph. Setting \texttt{requires\_grad} attribute to \texttt{true} allows differentiation w.r.t. to this value's position in the \texttt{autograd} graph. Supports almost all operations as \texttt{Tensor}s, except a few.
\item \texttt{Storage}: A data wrapper. It is a one-dimensional \emph{contiguous} set of data in memory. Typically, a \texttt{Tensor} contains multiple of such \texttt{Storage} blocks.
\end{itemize}
\end{flushleft}

\section*{Question 2}
\subsection*{Part a}
\begin{enumerate}
\item Modifying the learning rate produced different results.
\item When the learning rate was too high e.g., \texttt{lr=1e-02}, the losses diverged and quickly became \texttt{nan}.
\item When the learning rate was too low e.g., \texttt{lr=1e-04}, the loss converged slowly, which affected the quality of predictions.
\end{enumerate}

\subsection*{Part b}
\begin{center}
\begin{tabular}{|c|c|}
\hline
\textbf{Parameter} & \textbf{Value} \\
\hline
Weight 1 & 0.4175 \\
\hline
Weight 2 & 0.9582 \\
\hline
Bias & 35.3950 \\
\hline
\end{tabular}

\begin{tabular}{|c|c|}
\hline
\textbf{Test datapoint} & \textbf{Predicted value (Rounded off in brackets)}\\
\hline
\texttt{[6, 4]} & 41.7330 (42) \\
\hline
\texttt{[10, 5]} & 44.3613 (44) \\
\hline
\texttt{[14, 8]} & 48.9061 (49) \\
\hline
\end{tabular}
\end{center}

\subsection*{Part c}
\begin{itemize}
\item There were two versions implemented, least squares regression without bias and with bias (by appending 1 in the input data).
\item Predictions and weights of least squares regression without bias seemed to quite different from those obtained using SGD / least squares regression with bias.
\item The values are affected by the lack of the bias. The predictions and weights are displayed below.
\end{itemize}

\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Parameter Values} & \textbf{Weight 1} & \textbf{Weight 2} & \textbf{Bias} \\
\hline
SGD & 0.4175 & 0.9582 & 35.3950 \\
\hline
LSR (without bias) & 4.8018 & -2.6482 & N/A \\
\hline
LSR (with bias) & 0.6501 & 1.1099 & 31.9806 \\
\hline
\end{tabular}

\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Predictions (rounded off in brackets)} & \texttt{[6, 4]} & \texttt{[10, 5]} & \texttt{[14, 8]} \\
\hline
SGD & 41.7330 (42) & 44.3613 (44) & 48.9061 (49) \\
\hline
LSR (without bias) & 18.2181 (18) & 34.7772 (35) & 46.0399 (46) \\
\hline
LSR (with bias) & 40.3205 (40) & 44.0305 (44) & 49.9604 (50) \\
\hline
\end{tabular}
\end{center}

\section*{Question 3}
\subsection*{Part a}
\begin{flushleft}
The code snippet being referred to here are lines 105-140 (inclusive) and lines 159-EOF in the \texttt{mnist.py} file. The code does the following:
\begin{itemize}
\item Between lines 105-140, every time \texttt{test()} is called, the training accuracy and training loss over the entire dataset is computed. After this, these values are stored in variables \texttt{train\_loss} and \texttt{train\_accu}.
\item After the above step, the testing accuracy and testing loss over the entire dataset is computed. These values are stored in variables \texttt{test\_loss} and \texttt{test\_accu}, and returned collectively with the values from the training set.
\item From line 159 onwards, the losses are concatenated from each call to the \texttt{test()} function and then plotted using \texttt{matplotlib.pyplot}. Training and testing Losses are plotted on the left hand side of the graph and training and testing accuracies are plotted on the right hand side of the graph.
\end{itemize}
\end{flushleft}

\subsection*{Part b}
\begin{flushleft}
The best SGD criterion is:
\begin{itemize}
\item Batch Size (chosen from 48, 64, 92, 128, 144, 160): 64
\item Learning Rate (taken from the set \texttt{\{1e-04, 2e-04, 5e-04, 1e-03, 2e-03, 5e-03, 1e-02, 2e-02, 5e-02, 1e-01\}}): \texttt{0.001}
\item Momentum Parameter (taken from the set \texttt{\{0.0, 0.9, 0.95, 0.975, 0.99\}}): \texttt{0.975}
\item Weight Decay (taken from the set \texttt{\{0.0, 1e-04, 5e-04, 1e-03, 5e-03\}}): \texttt{1e-04}
\item Nesterov criterion: \texttt{False}
\end{itemize}

These values were chosen on the basis of both the rate of decrease of training loss and rate of increase of test classification error, while checking for generalizability. The code snippet for the classification error is as follows:
\begin{lstlisting}[language=Python]
output = model(input)
pred_classes = output.max(1)[1]
# below line computes absolute classification error
error = len(output) - torch.eq(pred_classes, target).sum()
\end{lstlisting}
\end{flushleft}

\subsection*{Part c}
\begin{flushleft}
The two other optimizers that I took were Adam and Adagrad. Adagrad has fewer parameters to perform a search over, effectively ``reducing'' the search space. On the other hand, Adam has a few more hyperparameters that can be configured, which would effectively take a lot of time. Adam has a few default values, but there are certain applications such as GANs where these default values don't give as good results. Hence, with the benefit of doubt, one can configure more parameters, which is unfortunately highly tedious.

Incorporating these for testing was done in the enclosed \texttt{mnist.py} file as well.
\end{flushleft}
\end{document}
