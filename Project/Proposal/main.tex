\documentclass{article}
\PassOptionsToPackage{numbers}{natbib}
\usepackage{optml}  % Using a mildly modified version of the style file provided for OPT-ML 2017 @ NIPS 2017
\usepackage{amsmath}
\usepackage[round]{natbib}
\bibliographystyle{plainnat}
\usepackage{hyperref}

\newcommand{\argmin}{\mathop{\mathrm{argmin}}}

\title{\rule{\linewidth}{2pt}\\\vspace{2mm}
Can random choices alleviate the burden of hyper-parameter optimization and model selection for neural networks?\\
\rule{\linewidth}{2pt}}
\author{\name{S. Vishwak}\email{cs15btech11043}\\
\name{Harsh Agarwal}\email{cs15btech11019}}

\begin{document}
\maketitle

\section{Introduction}
A critical step in hyper-parameter optimization and model selection in neural networks is to make a set of choices. These choices could be the learning rate, number of layers in a neural network, number of nodes in each layer of the neural network, weight decay parameters and so on. If we were to denote these set of choices by \(\Lambda\), then the goal of hyper-parameter optimization and model selection is to find the ideal \(\lambda^{*} \in \Lambda\) such that given a hyper-parameter objective \(\Psi\) (which could be cross-validation loss),
\begin{equation}
\lambda^{*} = \argmin_{\lambda \in \Lambda} \Psi(\lambda)
\end{equation}

Generally, if provided with \(K\) such configurable parameters, then \(\Lambda\) would be a set of \(K\)-tuples. Let the set of ``valid'' choices for each of the \(K\) configurable (hyper)parameters be \(C_{i}\), \(\forall i \in \{1, \ldots, K\}\). Now, the space of valid parameter combinations is \(\displaystyle \prod_{i=1}^{K} C_{i}\), which is affected by the curse of dimensionality.
\(\newline\)

There have been multiple attempts at improving hyperparameter optimization over the years. The most commonly used method is \textbf{grid search}, which is embarrasingly parallel, and hence can be done ``efficiently''. However, the task of determining the sets \(\displaystyle \lbrace C_{i} \rbrace_{i=1}^{K}\) remains, for which \textbf{manual search} is used, where one can identify regions in \(\Lambda\) that are more productive and promising, and these regions can be used to build the sets \(\displaystyle \lbrace C_{i} \rbrace_{i=1}^{K}\). Recently \citet{random-search} have provided an algorithm for hyper-parameter optimization called \textbf{random search}, which suggests that a search based entirely on random events is capable of finding, over the same domain \(\Lambda\), hyper-parameters and models which are just as good or even better than the ones found by grid search, with a fraction of the computation time and consequently energy.

\section{Proposed Methodology and Contributions}
In this project, we expect on contributions to be two-fold:
\begin{itemize}
\item [\emph{Con. 1}] Build a framework for random search proposed by \citet{random-search} for \texttt{PyTorch} for optimizing hyper-parameters
\item [\emph{Con. 2}] Check if randomly built models are collectively able to perform better as an ensemble than those models designed with extremely careful hyper-parameter and model choices.
\end{itemize}

\emph{Con. 1} has not been attempted before. In addition to considering canonical hyper-parameter choices made in general deep learning tasks such as learning rate, weight decay parameters, architecture specifications (to mention a few), we will also make choices over the weight initialization schemes (e.g. by \citet{glorot}, \citet{he}), optimization algorithms (e.g. AdaGrad(\citet{adagrad}), Adam(\citet{adam}), RMSProp(\citet{rmsprop})), pre-processing techniques and so on. The idea is to be able to construct an API / wrapper for \texttt{PyTorch} to do this automatically.
\(\newline\)

\emph{Con. 2} is a spin-off idea. The intuition behind this arises from Boosting, where a group of weak-classifiers can become a strong classifier. So we would like to tests the hypothesis that poor hyper-parameter optimized models when combined could be ``better'' than a well-studied model.

\section{Experiments to be conducted and Datasets to be used}
Our experiments will involve multi-layer perceptrons (at least) with Batch Normalization(\citet{batchnorm}) and Dropout(\citet{dropout}), of arbitrarily chosen sizes. Frameworks to be used are \texttt{PyTorch} and \texttt{scikit-learn}. \texttt{PyTorch} will be used for \emph{Con. 1} and \texttt{scikit-learn} will be used for \emph{Con. 2} where we have stable APIs for boosting and running multi-layer perceptrons.

Our experiments will involve the usage of 3 datasets - MNIST\footnote{\url{http://yann.lecun.com/exdb/mnist/}}, CIFAR10\footnote{\url{https://www.cs.toronto.edu/~kriz/cifar.html}} and SVHN\footnote{\url{http://ufldl.stanford.edu/housenumbers/}}. Since CIFAR10 and SVHN are extremely high-dimensional when flattened out, we will using feature extracted versions of those datasets obtained from Wide-Resnets. If time permits, we are also interested in obtaining results from randomly generated datasets as well. We also will be using a series of datasets with many factors of variation\footnote{Datasets available here: \url{https://www.iro.umontreal.ca/~lisa/twiki/bin/view.cgi/Public/DeepVsShallowComparisonICML2007}} which are used in the experiments conducted by \citet{grid-search} and \citet{random-search}.

\section{Expected Results and Performance Metrics}
For \emph{Con. 1}, we would like to verify the claim put forward by \citet{random-search} that random search is indeed faster than the ordinary grid search. For this, we will be noting the number of trials, run-time and system specifications and the final results obtained -training and testing loss and/or training and testing accuracy, and comparing the effectiveness of these two search methods. For \emph{Con. 2}, we will be comparing results with existing state-of-the-art results using just multi-layer perceptrons accuracy-wise.

\bibliography{main}
\end{document}
